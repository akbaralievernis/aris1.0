"""
–ú–µ–Ω–µ–¥–∂–µ—Ä ML –º–æ–¥–µ–ª–µ–π –¥–ª—è ARIS Neuro
–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–æ–π, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π
"""

import os
import sys
import json
import logging
import hashlib
import shutil
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import threading
import pickle

try:
    import torch
    import torch.nn as nn
    import numpy as np
    import whisper
    from transformers import AutoModel, AutoTokenizer
    import onnxruntime as ort
except ImportError as e:
    print(f"‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: {e}")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install -r requirements.txt")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class ModelInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    name: str
    type: str  # whisper, tts, wakeword, emotion, diarization
    version: str
    path: str
    size_mb: float
    loaded: bool = False
    device: str = "cpu"
    memory_mb: float = 0.0
    load_time: float = 0.0
    last_used: Optional[datetime] = None
    metadata: Dict = None

class ModelManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è ML –º–æ–¥–µ–ª—è–º–∏"""
    
    def __init__(self, models_dir: str = None, cache_dir: str = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π
        
        Args:
            models_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –º–æ–¥–µ–ª—è–º–∏
            cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∞
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.models_dir = Path(models_dir or os.getenv('MODELS_DIR', '/app/models'))
        self.cache_dir = Path(cache_dir or os.getenv('CACHE_DIR', '/app/cache/models'))
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.models_dir.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self.loaded_models: Dict[str, Any] = {}
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö
        self.models_info: Dict[str, ModelInfo] = {}
        
        # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        self.lock = threading.RLock()
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.config = {
            'max_loaded_models': 10,
            'auto_unload_timeout': 3600,  # 1 —á–∞—Å
            'cache_enabled': True,
            'gpu_memory_limit': 0.8,  # 80% GPU –ø–∞–º—è—Ç–∏
            'preload_models': ['whisper-medium', 'tts-default']
        }
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö
        self._load_models_info()
        
        logger.info(f"ModelManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ú–æ–¥–µ–ª–∏: {self.models_dir}, –ö—ç—à: {self.cache_dir}")
    
    def _load_models_info(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö"""
        info_file = self.cache_dir / 'models_info.json'
        
        if info_file.exists():
            try:
                with open(info_file, 'r') as f:
                    data = json.load(f)
                    for name, info_dict in data.items():
                        self.models_info[name] = ModelInfo(**info_dict)
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {len(self.models_info)} –º–æ–¥–µ–ª—è—Ö")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö: {e}")
    
    def _save_models_info(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö"""
        info_file = self.cache_dir / 'models_info.json'
        
        try:
            data = {
                name: asdict(info) 
                for name, info in self.models_info.items()
            }
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º datetime –≤ —Å—Ç—Ä–æ–∫–∏
            for name, info_dict in data.items():
                if info_dict.get('last_used'):
                    info_dict['last_used'] = info_dict['last_used'].isoformat()
            
            with open(info_file, 'w') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö: {e}")
    
    def register_model(
        self,
        name: str,
        model_type: str,
        path: str,
        version: str = "1.0.0",
        metadata: Dict = None
    ) -> ModelInfo:
        """
        –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        
        Args:
            name: –ò–º—è –º–æ–¥–µ–ª–∏
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏
            path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
            version: –í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            
        Returns:
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        """
        with self.lock:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
            model_path = Path(path)
            if not model_path.exists():
                raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {path}")
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä
            size_mb = model_path.stat().st_size / (1024 * 1024)
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
            model_info = ModelInfo(
                name=name,
                type=model_type,
                version=version,
                path=str(model_path.absolute()),
                size_mb=size_mb,
                metadata=metadata or {}
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            self.models_info[name] = model_info
            self._save_models_info()
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞: {name} ({model_type}, {size_mb:.2f} MB)")
            
            return model_info
    
    def load_model(
        self,
        name: str,
        device: str = None,
        force_reload: bool = False
    ) -> Any:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        
        Args:
            name: –ò–º—è –º–æ–¥–µ–ª–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cuda, cpu, auto)
            force_reload: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞
            
        Returns:
            –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        """
        with self.lock:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ —É–∂–µ –º–æ–¥–µ–ª—å
            if name in self.loaded_models and not force_reload:
                model_info = self.models_info.get(name)
                if model_info:
                    model_info.last_used = datetime.now()
                logger.debug(f"–ú–æ–¥–µ–ª—å {name} —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return self.loaded_models[name]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
            if name not in self.models_info:
                raise ValueError(f"–ú–æ–¥–µ–ª—å {name} –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞")
            
            model_info = self.models_info[name]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if device is None:
                device = "cuda" if torch.cuda.is_available() else "cpu"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            if len(self.loaded_models) >= self.config['max_loaded_models']:
                self._unload_oldest_model()
            
            start_time = time.time()
            
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
                model = self._load_model_by_type(model_info, device)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                model_info.loaded = True
                model_info.device = device
                model_info.load_time = time.time() - start_time
                model_info.last_used = datetime.now()
                
                # –í—ã—á–∏—Å–ª—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
                if device == "cuda" and torch.cuda.is_available():
                    model_info.memory_mb = torch.cuda.memory_allocated() / (1024 * 1024)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
                self.loaded_models[name] = model
                self._save_models_info()
                
                logger.info(
                    f"‚úÖ –ú–æ–¥–µ–ª—å {name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device} "
                    f"–∑–∞ {model_info.load_time:.2f} —Å–µ–∫"
                )
                
                return model
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {name}: {e}")
                raise
    
    def _load_model_by_type(self, model_info: ModelInfo, device: str) -> Any:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ —Ç–∏–ø—É"""
        model_type = model_info.type.lower()
        model_path = Path(model_info.path)
        
        if model_type == "whisper":
            # –ó–∞–≥—Ä—É–∑–∫–∞ Whisper –º–æ–¥–µ–ª–∏
            model_name = model_info.metadata.get('whisper_model', 'medium')
            model = whisper.load_model(
                name=model_name,
                device=device,
                download_root=str(self.models_dir / "whisper")
            )
            
        elif model_type == "tts":
            # –ó–∞–≥—Ä—É–∑–∫–∞ TTS –º–æ–¥–µ–ª–∏
            # –ó–¥–µ—Å—å –º–æ–∂–µ—Ç –±—ã—Ç—å Coqui TTS, VITS –∏ —Ç.–¥.
            if model_path.suffix in ['.pt', '.pth']:
                model = torch.load(model_path, map_location=device)
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º transformers –¥–ª—è TTS
                from TTS.api import TTS
                model = TTS(model_name=model_info.metadata.get('tts_model', 'tts_models/ru/ru_ruslan'))
            
        elif model_type == "wakeword":
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ wake word detection
            if model_path.suffix in ['.pt', '.pth']:
                model = torch.jit.load(str(model_path), map_location=device)
                model.eval()
            elif model_path.suffix == '.onnx':
                model = ort.InferenceSession(str(model_path))
            elif model_path.suffix == '.tflite':
                import tflite_runtime.interpreter as tflite
                model = tflite.Interpreter(model_path=str(model_path))
                model.allocate_tensors()
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –º–æ–¥–µ–ª–∏: {model_path.suffix}")
            
        elif model_type == "emotion":
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ —ç–º–æ—Ü–∏–π
            from transformers import pipeline
            model = pipeline(
                "audio-classification",
                model=model_info.metadata.get('emotion_model', 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'),
                device=0 if device == "cuda" else -1
            )
            
        elif model_type == "diarization":
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            from pyannote.audio import Pipeline
            model = Pipeline.from_pretrained(
                model_info.metadata.get('diarization_model', 'pyannote/speaker-diarization'),
                use_auth_token=os.getenv("HUGGINGFACE_TOKEN")
            ).to(torch.device(device))
            
        elif model_type == "pytorch":
            # –û–±—â–∞—è PyTorch –º–æ–¥–µ–ª—å
            model = torch.load(model_path, map_location=device)
            if hasattr(model, 'eval'):
                model.eval()
                
        elif model_type == "onnx":
            # ONNX –º–æ–¥–µ–ª—å
            model = ort.InferenceSession(
                str(model_path),
                providers=['CUDAExecutionProvider', 'CPUExecutionProvider'] if device == "cuda" else ['CPUExecutionProvider']
            )
            
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")
        
        return model
    
    def unload_model(self, name: str) -> bool:
        """
        –í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏
        
        Args:
            name: –ò–º—è –º–æ–¥–µ–ª–∏
            
        Returns:
            True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –±—ã–ª–∞ –≤—ã–≥—Ä—É–∂–µ–Ω–∞
        """
        with self.lock:
            if name not in self.loaded_models:
                logger.warning(f"–ú–æ–¥–µ–ª—å {name} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return False
            
            try:
                model = self.loaded_models[name]
                
                # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å GPU –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
                if self.models_info[name].device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # –£–¥–∞–ª—è–µ–º –º–æ–¥–µ–ª—å
                del self.loaded_models[name]
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                if name in self.models_info:
                    self.models_info[name].loaded = False
                    self.models_info[name].memory_mb = 0.0
                
                self._save_models_info()
                
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {name} –≤—ã–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –ø–∞–º—è—Ç–∏")
                return True
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {name}: {e}")
                return False
    
    def _unload_oldest_model(self):
        """–í—ã–≥—Ä—É–∑–∫–∞ —Å–∞–º–æ–π —Å—Ç–∞—Ä–æ–π –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –º–æ–¥–µ–ª–∏"""
        if not self.loaded_models:
            return
        
        # –ù–∞—Ö–æ–¥–∏–º –º–æ–¥–µ–ª—å —Å —Å–∞–º—ã–º —Å—Ç–∞—Ä—ã–º –≤—Ä–µ–º–µ–Ω–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        oldest_name = None
        oldest_time = None
        
        for name, model_info in self.models_info.items():
            if name in self.loaded_models and model_info.last_used:
                if oldest_time is None or model_info.last_used < oldest_time:
                    oldest_time = model_info.last_used
                    oldest_name = name
        
        if oldest_name:
            logger.info(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤—ã–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏: {oldest_name}")
            self.unload_model(oldest_name)
    
    def get_model(self, name: str) -> Optional[Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        
        Args:
            name: –ò–º—è –º–æ–¥–µ–ª–∏
            
        Returns:
            –ú–æ–¥–µ–ª—å –∏–ª–∏ None
        """
        with self.lock:
            return self.loaded_models.get(name)
    
    def get_model_info(self, name: str) -> Optional[ModelInfo]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
        
        Args:
            name: –ò–º—è –º–æ–¥–µ–ª–∏
            
        Returns:
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ –∏–ª–∏ None
        """
        return self.models_info.get(name)
    
    def list_models(self, model_type: str = None) -> List[ModelInfo]:
        """
        –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        
        Args:
            model_type: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –º–æ–¥–µ–ª–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö
        """
        models = list(self.models_info.values())
        
        if model_type:
            models = [m for m in models if m.type == model_type]
        
        return models
    
    def list_loaded_models(self) -> List[str]:
        """–°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return list(self.loaded_models.keys())
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π"""
        total_models = len(self.models_info)
        loaded_models = len(self.loaded_models)
        
        total_size = sum(m.size_mb for m in self.models_info.values())
        loaded_size = sum(
            m.memory_mb for m in self.models_info.values() 
            if m.name in self.loaded_models
        )
        
        models_by_type = {}
        for model_info in self.models_info.values():
            model_type = model_info.type
            models_by_type[model_type] = models_by_type.get(model_type, 0) + 1
        
        return {
            'total_models': total_models,
            'loaded_models': loaded_models,
            'total_size_mb': total_size,
            'loaded_size_mb': loaded_size,
            'models_by_type': models_by_type,
            'cache_enabled': self.config['cache_enabled'],
            'max_loaded_models': self.config['max_loaded_models']
        }
    
    def cleanup_unused_models(self, max_age_seconds: int = None):
        """
        –û—á–∏—Å—Ç–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π
        
        Args:
            max_age_seconds: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç –Ω–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        """
        if max_age_seconds is None:
            max_age_seconds = self.config['auto_unload_timeout']
        
        current_time = datetime.now()
        unloaded = 0
        
        with self.lock:
            for name, model_info in self.models_info.items():
                if name in self.loaded_models and model_info.last_used:
                    age = (current_time - model_info.last_used).total_seconds()
                    if age > max_age_seconds:
                        self.unload_model(name)
                        unloaded += 1
        
        if unloaded > 0:
            logger.info(f"–û—á–∏—â–µ–Ω–æ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π: {unloaded}")
    
    def preload_models(self, model_names: List[str] = None):
        """
        –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
        
        Args:
            model_names: –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏
        """
        if model_names is None:
            model_names = self.config['preload_models']
        
        logger.info(f"–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π: {model_names}")
        
        for name in model_names:
            try:
                if name in self.models_info:
                    self.load_model(name)
                else:
                    logger.warning(f"–ú–æ–¥–µ–ª—å {name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {name}: {e}")
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"""
        logger.info("–û—á–∏—Å—Ç–∫–∞ ModelManager...")
        
        with self.lock:
            # –í—ã–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
            for name in list(self.loaded_models.keys()):
                self.unload_model(name)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            self._save_models_info()
        
        logger.info("‚úÖ ModelManager –æ—á–∏—â–µ–Ω")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞
_manager_instance = None

def get_manager() -> ModelManager:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –º–µ–Ω–µ–¥–∂–µ—Ä–∞"""
    global _manager_instance
    if _manager_instance is None:
        _manager_instance = ModelManager()
    return _manager_instance

# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Model Manager –¥–ª—è ARIS Neuro")
    parser.add_argument("--action", choices=["list", "load", "unload", "stats", "register"], default="list")
    parser.add_argument("--name", help="–ò–º—è –º–æ–¥–µ–ª–∏")
    parser.add_argument("--type", help="–¢–∏–ø –º–æ–¥–µ–ª–∏")
    parser.add_argument("--path", help="–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏")
    parser.add_argument("--device", choices=["cuda", "cpu", "auto"], default="auto")
    
    args = parser.parse_args()
    
    manager = get_manager()
    
    if args.action == "list":
        models = manager.list_models()
        print(f"\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ ({len(models)}):")
        for model in models:
            status = "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞" if model.loaded else "‚è∏Ô∏è  –ù–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
            print(f"  - {model.name} ({model.type}) - {status}")
    
    elif args.action == "load":
        if not args.name:
            print("‚ùå –£–∫–∞–∂–∏—Ç–µ –∏–º—è –º–æ–¥–µ–ª–∏: --name MODEL_NAME")
            sys.exit(1)
        try:
            model = manager.load_model(args.name, args.device)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {args.name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            sys.exit(1)
    
    elif args.action == "unload":
        if not args.name:
            print("‚ùå –£–∫–∞–∂–∏—Ç–µ –∏–º—è –º–æ–¥–µ–ª–∏: --name MODEL_NAME")
            sys.exit(1)
        success = manager.unload_model(args.name)
        if success:
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {args.name} –≤—ã–≥—Ä—É–∂–µ–Ω–∞")
        else:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {args.name}")
            sys.exit(1)
    
    elif args.action == "stats":
        stats = manager.get_stats()
        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ModelManager:")
        print(json.dumps(stats, indent=2, ensure_ascii=False))
    
    elif args.action == "register":
        if not all([args.name, args.type, args.path]):
            print("‚ùå –£–∫–∞–∂–∏—Ç–µ --name, --type –∏ --path")
            sys.exit(1)
        try:
            model_info = manager.register_model(args.name, args.type, args.path)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞: {model_info.name}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            sys.exit(1)
